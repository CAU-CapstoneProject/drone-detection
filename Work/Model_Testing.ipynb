{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import glob\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = joblib.load(\"11012018_randomforest_drones_background_n_mfcc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Test filest:  4\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "test_path = \"/Users/junhyuckwoo/capstone/TestFile/Data/DistanceTest/Syma/*.wav\"\n",
    "test_files = glob.glob(test_path)\n",
    "files_num = len(test_files)\n",
    "print(\"# Test filest: \", files_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape (87, 13)\n"
     ]
    }
   ],
   "source": [
    "# Input daata Preprocessing\n",
    "raw, sr = librosa.load(test_files[0], sr=44100)\n",
    "norm = maxabs_scale(raw[:44100])\n",
    "data= librosa.feature.mfcc(norm, sr=44100, n_mfcc=13).T\n",
    "# Checking for debugging\n",
    "print(\"Data Shape\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Testing data\n",
    "for index in range(1, files_num):\n",
    "    raw, sr = librosa.load(drone_files[index], sr=44100)\n",
    "    norm = maxabs_scale(raw)\n",
    "    mfcc_drone = librosa.feature.spectral_contrast(norm).T\n",
    "    #mfcc_drone= librosa.feature.mfcc(zc[0], sr=44100, n_mfcc=13).T\n",
    "    drone = np.concatenate((drone, mfcc_drone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.ones(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data\n",
    "y = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 10282018_randomforest_bee2drone_n_sc.pkl\n",
      "F-Score: 0.989\n",
      "Accuracy:  0.9885057471264368\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         0\n",
      "        1.0       1.00      0.99      0.99        87\n",
      "\n",
      "avg / total       1.00      0.99      0.99        87\n",
      "\n",
      "[[ 0  0]\n",
      " [ 1 86]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junhyuckwoo/capstone/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "p,r,f,s = precision_recall_fscore_support(y, prediction, average='micro')\n",
    "print(\"Model: 10282018_randomforest_bee2drone_n_sc.pkl\")\n",
    "print(\"F-Score:\", round(f,3))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: \", accuracy_score(y, prediction))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, prediction))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
